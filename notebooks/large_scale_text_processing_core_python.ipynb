{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9b9ccd-7610-4b37-b3e3-d7edeb134245",
   "metadata": {},
   "source": [
    "# Large-scale Text Processing in Core Python\n",
    "\n",
    "Portfolio notebook demonstrating efficient processing of large text datasets using core Python only (no Pandas/NumPy/NLP libraries).  \n",
    "Focus: streaming I/O, memory-aware data structures, and algorithmic text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916beb2-7f91-4399-b9b1-d7116b60f130",
   "metadata": {},
   "source": [
    "## What’s inside\n",
    "- Streaming and parsing line-delimited JSON datasets (Wikipedia-style articles + tweets)\n",
    "- Text normalisation/token handling\n",
    "- Frequency analysis and case-variant tracking\n",
    "- Similarity matching using Jaccard index (with pruning)\n",
    "- Building an inverted index and supporting wildcard-style search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6578f-608f-4b80-842e-3074b9898f97",
   "metadata": {},
   "source": [
    "## Data\n",
    "This notebook expects the following files to be available locally in a `data/` directory (not committed to GitHub due to size and licensing constraints):\n",
    "\n",
    "- `wiki-articles.json`\n",
    "- `tweets.json`\n",
    "- `linuxwords`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76476f5-4a54-4ec1-aaa5-5d287bc000bb",
   "metadata": {},
   "source": [
    "## Notebook structure\n",
    "1. Wikipedia corpus analysis\n",
    "2. Twitter corpus analysis\n",
    "3. Cross-corpus matching (Jaccard similarity)\n",
    "4. Inverted index + wildcard search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e431d1-2037-4688-8d4a-ee8afed99b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & utilities\n",
    "# - Resolve data directory for both university JupyterHub and GitHub layouts\n",
    "# - Define shared text-processing helpers used throughout the notebook\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Resolve data directory\n",
    "DATA_DIR = Path.cwd()\n",
    "if not (DATA_DIR / \"wiki-articles.json\").exists():\n",
    "    DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "# Note: this translator removes all punctuation.\n",
    "# Twitter-specific processing (e.g. @mentions) is handled separately where needed.\n",
    "TRANSLATOR = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def normalise(text: str) -> list[str]:\n",
    "    \"\"\"Lowercase, strip punctuation, and split on whitespace.\"\"\"\n",
    "    return text.translate(TRANSLATOR).lower().split()\n",
    "\n",
    "def tokens_from_json_line(obj: dict, field: str) -> list[str]:\n",
    "    \"\"\"Extract and normalise tokens from a JSON object field.\"\"\"\n",
    "    value = obj.get(field, \"\")\n",
    "    return normalise(value) if isinstance(value, str) else []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474863e-5775-4632-901c-eb1a3f7adc7c",
   "metadata": {},
   "source": [
    "**Note**: Tokenisation here is intentionally simple (whitespace-based, punctuation stripped) to keep the focus on algorithmic text processing rather than linguistic accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f133d-4f4b-4fd7-a287-e2078c8eb86e",
   "metadata": {},
   "source": [
    "## 1) Wikipedia: Most frequent unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c086b-3068-4b78-8bf8-2dd6571a0fd7",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify the most frequently occurring words across the Wikipedia article corpus, counting each word at most once per article.\n",
    "\n",
    "This approach avoids overweighting articles that repeat the same term many times and instead highlights words that appear broadly across the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615c7b88-e61d-4ef9-a878-6103843c3c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent words across articles (unique per article):\n",
      "was             17131\n",
      "from            16460\n",
      "that            16328\n",
      "this            16173\n",
      "which           16027\n",
      "also            15970\n",
      "are             15312\n",
      "has             15159\n",
      "one             14850\n",
      "other           14828\n"
     ]
    }
   ],
   "source": [
    "# 1) Wikipedia: Most frequent unique words\n",
    "\n",
    "# Count of unique word occurrences across articles\n",
    "word_counts = Counter()\n",
    "\n",
    "with open(DATA_DIR / \"wiki-articles.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        article = json.loads(line)\n",
    "        text = article.get(\"text\", \"\")\n",
    "        unique_words = set(normalise(text))  # count each token at most once per article\n",
    "        word_counts.update(unique_words)\n",
    "\n",
    "# Filter very common stopwords for a more informative top list\n",
    "STOPWORDS = {\"the\", \"and\", \"of\", \"to\", \"in\", \"a\", \"is\", \"for\", \"on\", \"with\", \"what\"}\n",
    "\n",
    "filtered_counts = Counter(\n",
    "    {word: count for word, count in word_counts.items() if word not in STOPWORDS and len(word) > 2}\n",
    ")\n",
    "\n",
    "print(\"Top 10 most frequent words across articles (unique per article):\")\n",
    "for word, count in filtered_counts.most_common(10):\n",
    "    print(f\"{word:<15} {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d87c14-c1a1-4424-8f4a-deab7a5236fc",
   "metadata": {},
   "source": [
    "## 2) Wikipedia: Longest capitalised phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5563884d-ddf3-42ab-9278-881609c21a05",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify the longest contiguous sequence of capitalised words appearing in Wikipedia article body text, where a word is considered capitalised if its first character is uppercase.\n",
    "\n",
    "For example, in the sentence  \n",
    "“Queen Mary University is located in the United Kingdom, specifically in London”,  \n",
    "the capitalised sequences are “Queen Mary University”, “United Kingdom”, and “London”.  \n",
    "The longest sequence in this case is “Queen Mary University”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0b6330-0bcd-4b95-8d11-fa845e7e89eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence length: 134 words\n",
      "Found in article: Humphrey Bogart\n",
      "Preview: Havilland Michael Curtiz James Cagney David O Selznick William Wyler Richard Brooks Harry Cohn Jane Wyman Jean Arthur Claude Rains Raymond Massey George Raft Myrna ...\n"
     ]
    }
   ],
   "source": [
    "# 2) Wikipedia: Longest capitalised phrase\n",
    "\n",
    "def is_capitalised_token(token: str) -> bool:\n",
    "    \"\"\"\n",
    "    A token is 'capitalised' if its first character is an uppercase letter.\n",
    "    We also ignore tokens that start with a digit or punctuation (after cleaning).\n",
    "    \"\"\"\n",
    "    return bool(token) and token[0].isalpha() and token[0].isupper()\n",
    "\n",
    "def longest_capitalised_sequence(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"Return the longest contiguous subsequence of capitalised tokens.\"\"\"\n",
    "    best_start = best_len = 0\n",
    "    cur_start = cur_len = 0\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if is_capitalised_token(tok):\n",
    "            if cur_len == 0:\n",
    "                cur_start = i\n",
    "            cur_len += 1\n",
    "\n",
    "            if cur_len > best_len:\n",
    "                best_len = cur_len\n",
    "                best_start = cur_start\n",
    "        else:\n",
    "            cur_len = 0\n",
    "\n",
    "    return tokens[best_start:best_start + best_len]\n",
    "\n",
    "best_sequence = []\n",
    "best_article_title = None\n",
    "\n",
    "with open(DATA_DIR / \"wiki-articles.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        article = json.loads(line)\n",
    "\n",
    "        # Keep original case (do NOT call normalise here, as it lowercases)\n",
    "        text = article.get(\"text\", \"\")\n",
    "        tokens = text.translate(TRANSLATOR).split()\n",
    "\n",
    "        seq = longest_capitalised_sequence(tokens)\n",
    "        if len(seq) > len(best_sequence):\n",
    "            best_sequence = seq\n",
    "            best_article_title = article.get(\"title\")\n",
    "\n",
    "# Reduce output spam: show preview + length + where it was found\n",
    "preview_n = 25\n",
    "preview = \" \".join(best_sequence[:preview_n])\n",
    "suffix = \" ...\" if len(best_sequence) > preview_n else \"\"\n",
    "\n",
    "print(f\"Longest sequence length: {len(best_sequence)} words\")\n",
    "print(f\"Found in article: {best_article_title}\")\n",
    "print(f\"Preview: {preview}{suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f3cf5-33ae-4aaf-b701-cb66ce752eeb",
   "metadata": {},
   "source": [
    "## 3) Wikipedia: Largest anagram set (filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc228570-a35e-451d-b395-688ffbeee63c",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify the largest set of anagrammatic words appearing in Wikipedia article body text, considering only words with at least six characters.\n",
    "\n",
    "Two words are anagrams if they consist of the same characters in a different order (including repeated characters). For example:\n",
    "- “cheap” and “peach”\n",
    "- “reacting” and “creating”\n",
    "- “resistance” and “ancestries”\n",
    "\n",
    "Anagram sets may contain more than two words. For instance:\n",
    "- {“asleep”, “please”, “elapse”} form a 3-word anagram set\n",
    "- {“aspired”, “despair”, “diapers”, “praised”} form a 4-word anagram set\n",
    "\n",
    "Words are collected across the entire article corpus (not limited to a single article), and only article body text is considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fdafc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest anagram set size: 19\n",
      "Example signature: aeimnr\n",
      "Words (showing up to 20):\n",
      "- airmen\n",
      "- armeni\n",
      "- ermina\n",
      "- mainer\n",
      "- maneri\n",
      "- manier\n",
      "- marien\n",
      "- marine\n",
      "- marnie\n",
      "- meirna\n",
      "- merian\n",
      "- merina\n",
      "- minera\n",
      "- mirena\n",
      "- namier\n",
      "- nerima\n",
      "- reiman\n",
      "- remain\n",
      "- rmaine\n"
     ]
    }
   ],
   "source": [
    "# 3) Wikipedia: Largest anagram set (filtered)\n",
    "\n",
    "def signature(word: str) -> str:\n",
    "    \"\"\"Canonical anagram signature: sorted characters.\"\"\"\n",
    "    return \"\".join(sorted(word))\n",
    "\n",
    "groups = defaultdict(set)\n",
    "\n",
    "with open(DATA_DIR / \"wiki-articles.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        article = json.loads(line)\n",
    "        text = article.get(\"text\", \"\")\n",
    "\n",
    "        for w in normalise(text):\n",
    "            # Filters to avoid tokenisation artefacts and \"non-words\"\n",
    "            # - length constraint from objective (>= 6)\n",
    "            # - alphabetic only (removes things like abcdef..., mixed tokens, etc.)\n",
    "            # - reasonable max length to avoid weird long strings dominating\n",
    "            if 6 <= len(w) <= 20 and w.isalpha():\n",
    "                groups[signature(w)].add(w)\n",
    "\n",
    "# Find the largest anagram group\n",
    "best_sig, best_group = max(groups.items(), key=lambda kv: len(kv[1]))\n",
    "\n",
    "best_sorted = sorted(best_group)\n",
    "\n",
    "print(f\"Largest anagram set size: {len(best_sorted)}\")\n",
    "print(f\"Example signature: {best_sig}\")\n",
    "print(\"Words (showing up to 20):\")\n",
    "for word in best_sorted[:20]:\n",
    "    print(f\"- {word}\")\n",
    "\n",
    "if len(best_sorted) > 20:\n",
    "    print(f\"... (+{len(best_sorted) - 20} more)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914ba6e-d487-47ff-88d9-bcf65c2c455a",
   "metadata": {},
   "source": [
    "## 4) Twitter: Top mentioned users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cee4d0-5cf0-405a-a9df-7268fbc459e3",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify the most frequently mentioned Twitter users by analysing tokens that begin with the `@` symbol, and report the top five users along with their mention counts.\n",
    "\n",
    "During tokenisation, punctuation is removed while preserving the `@` character so that user mentions are not discarded. While this approach is not perfect in all edge cases, it is sufficient for reliably extracting mentions from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b206fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mentioned users:\n",
      "@btstwt          31880\n",
      "@enhypenmembers  4702\n",
      "@enhypen         4169\n",
      "@blackpink       3891\n",
      "@nctsmtown       3450\n"
     ]
    }
   ],
   "source": [
    "# 4) Twitter: Top mentioned users\n",
    "\n",
    "# Create a translation table that removes punctuation EXCEPT '@'\n",
    "MENTION_TRANSLATOR = str.maketrans(\"\", \"\", string.punctuation.replace(\"@\", \"\"))\n",
    "\n",
    "mentions_by_user = Counter()\n",
    "\n",
    "with open(DATA_DIR / \"tweets.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        tweet_obj = json.loads(line)\n",
    "        tweet_text = tweet_obj.get(\"text\", \"\")\n",
    "\n",
    "        # Remove punctuation (except '@') and lowercase for consistent counting\n",
    "        cleaned = tweet_text.translate(MENTION_TRANSLATOR).lower()\n",
    "\n",
    "        # Extract @mentions and strip the '@'\n",
    "        for token in cleaned.split():\n",
    "            if token.startswith(\"@\") and len(token) > 1:\n",
    "                username = token[1:]\n",
    "                # Basic sanity filter: usernames should be alphanumeric/underscore\n",
    "                if username.replace(\"_\", \"\").isalnum():\n",
    "                    mentions_by_user[username] += 1\n",
    "\n",
    "print(\"Top 5 mentioned users:\")\n",
    "for username, count in mentions_by_user.most_common(5):\n",
    "    print(f\"@{username:<15} {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26892355-ad95-4098-923a-7672d14799a9",
   "metadata": {},
   "source": [
    "## 5) Twitter: Word with most case variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96dfa87-3678-4ef5-944c-3b51f2e41d9e",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify the word in the Twitter dataset that appears with the largest number of distinct case variations, while preserving the original casing of tokens.\n",
    "\n",
    "Case variations refer to tokens that differ only by letter casing (e.g. “house” vs “HouSe”) and are treated as the same underlying word when lowercased, but counted separately based on their original form.\n",
    "\n",
    "For example, given the tokens  \n",
    "“HouSe, House, HOUSE, YES, yes, YeS, Yes, yeS, yEs”,  \n",
    "the word “house” has 3 case variations and “yes” has 6.  \n",
    "In this example, “yes” would be identified as the word with the most case variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27425da2-db5b-4a67-9a97-0d1a2627b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word with most case variations (base form): 'this'\n",
      "Number of distinct case variations: 11\n",
      "Variations (unique):\n",
      "THIS, THiS, THis, ThIs, ThiS, This, tHIS, tHiS, thIS, thiS, this\n"
     ]
    }
   ],
   "source": [
    "# 5) Twitter: Word with most case variations\n",
    "\n",
    "# exclude very common stopwords so results are more informative\n",
    "STOPWORDS = {\"the\", \"and\", \"of\", \"to\", \"in\", \"a\", \"is\", \"for\", \"on\", \"with\", \"what\"}\n",
    "\n",
    "def iter_alpha_tokens_preserve_case(text: str):\n",
    "    \"\"\"\n",
    "    Yield alphabetic tokens while preserving original case.\n",
    "    Punctuation is stripped from token boundaries only.\n",
    "    \"\"\"\n",
    "    for raw in text.split():\n",
    "        token = raw.strip(string.punctuation)\n",
    "        if token.isalpha():\n",
    "            yield token\n",
    "\n",
    "# Map: lowercase base form -> set of observed case variants\n",
    "case_variants_by_base = defaultdict(set)\n",
    "\n",
    "# Track the best (most variants) incrementally\n",
    "best_base = None\n",
    "best_variants = set()\n",
    "best_count = 0\n",
    "\n",
    "with open(DATA_DIR / \"tweets.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet.get(\"text\", \"\")\n",
    "\n",
    "        for original in iter_alpha_tokens_preserve_case(text):\n",
    "            base = original.lower()\n",
    "\n",
    "            # Skip very common stopwords (for cleaner results)\n",
    "            if base in STOPWORDS:\n",
    "                continue\n",
    "\n",
    "            variants_set = case_variants_by_base[base]\n",
    "            before = len(variants_set)\n",
    "            variants_set.add(original)\n",
    "            after = len(variants_set)\n",
    "\n",
    "            # Update winner only when a new distinct variant is added\n",
    "            if after > before and after > best_count:\n",
    "                best_count = after\n",
    "                best_base = base\n",
    "                best_variants = variants_set\n",
    "\n",
    "# Clean, readable output\n",
    "sorted_variants = sorted(best_variants, key=lambda s: (s.lower(), s))\n",
    "\n",
    "print(f\"Word with most case variations (base form): '{best_base}'\")\n",
    "print(f\"Number of distinct case variations: {best_count}\")\n",
    "print(\"Variations (unique):\")\n",
    "print(\", \".join(sorted_variants))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab683620-96df-49b3-b2b3-e09f23b70713",
   "metadata": {},
   "source": [
    "## 6) Twitter: Longest dictionary-word sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8290dbb-f256-486f-89a9-481eddf1eeaf",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Identify the longest contiguous sequence of dictionary words appearing in the Twitter dataset that occurs in at least 10 distinct tweets.\n",
    "\n",
    "A dictionary word is defined as a token present in the provided English word list. Sequences are formed by grouping adjacent dictionary words within a tweet.\n",
    "\n",
    "For example, given the tweet  \n",
    "“word1 word2 word3 word4 word5 word6 word7”,  \n",
    "and a dictionary containing “word1”, “word3”, “word4”, and “word7”,  \n",
    "the resulting dictionary-word sequences are:\n",
    "- “word1”\n",
    "- “word3 word4”\n",
    "- “word7”\n",
    "\n",
    "In this example, the longest valid sequence is “word3 word4”. Only sequences meeting the minimum frequency threshold across the dataset are considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d12ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest dictionary-word sequence (>= 10 tweets):\n",
      "bonus fact fake friends and fans will turn on you at the very first rumor of drama the real will need tons of proof and will defend you\n",
      "Length (words): 28\n",
      "Distinct tweets: 17\n"
     ]
    }
   ],
   "source": [
    "# 6) Twitter: Longest dictionary-word sequence (>= 10 tweets)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def load_dictionary_words(path=None) -> set[str]:\n",
    "    \"\"\"Load dictionary words into a lowercase set for O(1) membership checks.\"\"\"\n",
    "    if path is None:\n",
    "        path = DATA_DIR / \"linuxwords\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        return {line.strip().lower() for line in fh if line.strip()}\n",
    "\n",
    "def iter_dictionary_runs(tokens: list[str], dict_words: set[str]):\n",
    "    \"\"\"\n",
    "    Yield maximal contiguous runs of dictionary words as tuples.\n",
    "    Example: tokens -> [\"a\",\"b\",\"c\"], where a,c in dict but b not -> yields (\"a\",), (\"c\",)\n",
    "    \"\"\"\n",
    "    run = []\n",
    "    for tok in tokens:\n",
    "        if tok in dict_words:\n",
    "            run.append(tok)\n",
    "        else:\n",
    "            if run:\n",
    "                yield tuple(run)\n",
    "                run = []\n",
    "    if run:\n",
    "        yield tuple(run)\n",
    "\n",
    "dict_words = load_dictionary_words()\n",
    "run_counts = Counter()  # run tuple -> number of distinct tweets it appears in\n",
    "\n",
    "with open(DATA_DIR / \"tweets.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet.get(\"text\", \"\")\n",
    "\n",
    "        # Lowercase + punctuation stripped (OK here; we are matching dictionary words)\n",
    "        toks = normalise(text)\n",
    "\n",
    "        # Count each run at most once per tweet\n",
    "        seen_runs = set(iter_dictionary_runs(toks, dict_words))\n",
    "        run_counts.update(seen_runs)\n",
    "\n",
    "# Find the longest run that appears in at least 10 tweets\n",
    "min_tweets = 10\n",
    "best_run = ()\n",
    "best_count = 0\n",
    "\n",
    "for run, c in run_counts.items():\n",
    "    if c >= min_tweets:\n",
    "        if len(run) > len(best_run) or (len(run) == len(best_run) and c > best_count):\n",
    "            best_run = run\n",
    "            best_count = c\n",
    "\n",
    "best_text = \" \".join(best_run)\n",
    "\n",
    "print(f\"Longest dictionary-word sequence (>= {min_tweets} tweets):\")\n",
    "print(best_text)\n",
    "print(f\"Length (words): {len(best_run)}\")\n",
    "print(f\"Distinct tweets: {best_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3ed4d-945b-48f9-845c-554ece0df9fd",
   "metadata": {},
   "source": [
    "## 7) Matching tweets to Wikipedia articles using Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0762e11-be5e-422a-ac41-fbb0e714d7a8",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Given a tweet, identify the Wikipedia article whose body text is most similar to the tweet content, using the Jaccard similarity index as the similarity metric.\n",
    "\n",
    "Similarity is computed by representing both the tweet and each article as sets of unique tokens (derived from article body text only), and selecting the article with the highest Jaccard score.\n",
    "\n",
    "The Jaccard index between two texts A and B is defined as:\n",
    "\n",
    "$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "For example, given:\n",
    "- A: “python is a fun programming language”\n",
    "- B: “programming with python is always fun”\n",
    "\n",
    "The intersection contains {python, is, fun, programming} and the union contains 8 unique tokens, yielding a Jaccard similarity of 0.5.\n",
    "\n",
    "This approach provides a simple and interpretable measure of lexical overlap, though it can be sensitive to differences in text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89978d9-58ac-4a3f-97f3-8831da2769a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match for tweet 1380411674478354434: article 32199 (Jaccard=0.115)\n"
     ]
    }
   ],
   "source": [
    "# 7) Matching tweets to Wikipedia articles using Jaccard similarity\n",
    "\n",
    "def to_word_set(text: str) -> set[str]:\n",
    "    \"\"\"Convert text to a set of unique normalised tokens.\"\"\"\n",
    "    return set(normalise(text))\n",
    "\n",
    "def jaccard_index(a: set[str], b: set[str]) -> float:\n",
    "    \"\"\"Compute J(A,B) = |A ∩ B| / |A ∪ B| efficiently.\"\"\"\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "    # Iterate over smaller set for fewer membership checks\n",
    "    if len(a) > len(b):\n",
    "        a, b = b, a\n",
    "    intersection = sum(1 for tok in a if tok in b)\n",
    "    union = len(a) + len(b) - intersection\n",
    "    return intersection / union if union else 0.0\n",
    "\n",
    "def best_match(tweet_id: str) -> tuple[int | None, float]:\n",
    "    \"\"\"\n",
    "    Given a tweet ID, return (best_article_id, best_jaccard_score)\n",
    "    comparing the tweet text against Wikipedia article body text only.\n",
    "    \"\"\"\n",
    "    # 1) Locate the tweet and build its word set\n",
    "    tweet_terms = None\n",
    "    with open(DATA_DIR / \"tweets.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet.get(\"id_str\") == tweet_id:\n",
    "                tweet_terms = to_word_set(tweet.get(\"text\", \"\"))\n",
    "                break\n",
    "\n",
    "    if tweet_terms is None:\n",
    "        return None, 0.0\n",
    "\n",
    "    tweet_len = len(tweet_terms)\n",
    "\n",
    "    # 2) Stream Wikipedia articles and keep the running best match\n",
    "    best_article_id = None\n",
    "    best_similarity = -1.0\n",
    "\n",
    "    with open(DATA_DIR / \"wiki-articles.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            article = json.loads(line)\n",
    "            article_terms = to_word_set(article.get(\"text\", \"\"))\n",
    "            article_len = len(article_terms)\n",
    "\n",
    "            # Upper bound on Jaccard given set sizes:\n",
    "            # |A∩B| <= min(|A|,|B|) and |A∪B| >= max(|A|,|B|)\n",
    "            # => J <= min(|A|,|B|) / max(|A|,|B|)\n",
    "            smaller = tweet_len if tweet_len <= article_len else article_len\n",
    "            larger = article_len if tweet_len <= article_len else tweet_len\n",
    "            jaccard_upper_bound = (smaller / larger) if larger else 0.0\n",
    "\n",
    "            if jaccard_upper_bound <= best_similarity:\n",
    "                continue  # prune weak candidates\n",
    "\n",
    "            similarity = jaccard_index(tweet_terms, article_terms)\n",
    "\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_article_id = article.get(\"id\")\n",
    "\n",
    "    return best_article_id, best_similarity\n",
    "\n",
    "# Demo call \n",
    "tweet_id = \"1380411674478354434\"\n",
    "article_id, score = best_match(tweet_id)\n",
    "\n",
    "if article_id is None:\n",
    "    print(f\"No tweet found for id={tweet_id}\")\n",
    "else:\n",
    "    print(f\"Best match for tweet {tweet_id}: article {article_id} (Jaccard={score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f84c1-11d6-4f62-add2-aa63b3278608",
   "metadata": {},
   "source": [
    "## 8) Inverted index and wildcard search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce0c49-0713-4b9b-848c-5a945ee1cedb",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Build a mini search capability across both corpora (Wikipedia articles + tweets) by creating a shared **positional inverted index**, then use it to support **wildcard-style phrase queries**.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 1: Positional inverted index\n",
    "To enable efficient retrieval without scanning every document, I construct an inverted index that maps each token to:\n",
    "- the source type (Wikipedia article or tweet)\n",
    "- the document ID\n",
    "- the token position(s) within that document\n",
    "\n",
    "Example (toy):\n",
    "If an article has ID 7 and text “python coding is so much fun”, then:\n",
    "- `coding` appears at position 1 in article 7\n",
    "- `fun` appears at position 5 in article 7\n",
    "\n",
    "This positional information enables phrase and pattern matching rather than simple keyword lookup.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Wildcard search over the index\n",
    "Using the positional index, I implement wildcard-style queries where `*` represents any single token.\n",
    "\n",
    "Example query:\n",
    "`word1 * * * word2 * word3`\n",
    "\n",
    "This matches documents where:\n",
    "- `word1` is followed by any 3 tokens,\n",
    "- then `word2`,\n",
    "- then any 1 token,\n",
    "- then `word3`.\n",
    "\n",
    "For this notebook, queries are assumed to start and end with a word (not `*`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4ee342-7909-4da6-934f-bf5bdbdb4ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: new * uk\n",
      "Articles matched: 2\n",
      "Article IDs (preview): ['30028', '32356'] \n",
      "Tweets matched: 14\n",
      "Tweet IDs (preview): ['1380448072661106690', '1380458625508999172', '1380458856187318275', '1380460043200565249', '1380463180552544256', '1380471913076625410', '1380474505152368648', '1380476241594224640', '1380484873455042560', '1380494818187563009'] ...\n",
      "------------------------------------------------------------\n",
      "Query: institute * * * university * * university * university\n",
      "Articles matched: 1\n",
      "Article IDs (preview): ['18879'] \n",
      "Tweets matched: 0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 8) Inverted index and wildcard search\n",
    "\n",
    "# -------------------------\n",
    "# Part 1: Positional inverted index\n",
    "# index[token][source_label][doc_id] -> list of positions\n",
    "# -------------------------\n",
    "\n",
    "index = defaultdict(lambda: {\"article\": defaultdict(list), \"tweet\": defaultdict(list)})\n",
    "\n",
    "def add_document_to_index(source_label: str, document_id, text: str) -> None:\n",
    "    \"\"\"Add token positions for a single document to the shared index.\"\"\"\n",
    "    tokens = normalise(text)  # lowercase + punctuation stripped (matches index keys)\n",
    "    postings_for_token = index  # local alias for speed in tight loop\n",
    "\n",
    "    for pos, tok in enumerate(tokens):\n",
    "        postings_for_token[tok][source_label][document_id].append(pos)\n",
    "\n",
    "# Build index: Wikipedia articles\n",
    "with open(DATA_DIR / \"wiki-articles.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        article = json.loads(line)\n",
    "        add_document_to_index(\"article\", article.get(\"id\"), article.get(\"text\", \"\"))\n",
    "\n",
    "# Build index: Tweets\n",
    "with open(DATA_DIR / \"tweets.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    for line in fh:\n",
    "        tweet = json.loads(line)\n",
    "        add_document_to_index(\"tweet\", tweet.get(\"id_str\"), tweet.get(\"text\", \"\"))\n",
    "\n",
    "# -------------------------\n",
    "# Part 2: Wildcard search over the index\n",
    "# -------------------------\n",
    "\n",
    "def parse_query(query_text: str) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Convert a wildcard query into:\n",
    "    - words: list of normalised query terms (excluding '*')\n",
    "    - gaps:  list where gaps[i] is the number of '*' between words[i] and words[i+1]\n",
    "    Assumes query starts and ends with a word (not '*').\n",
    "    \"\"\"\n",
    "    raw_tokens = query_text.split()\n",
    "\n",
    "    cleaned = []\n",
    "    for t in raw_tokens:\n",
    "        if t == \"*\":\n",
    "            cleaned.append(\"*\")\n",
    "        else:\n",
    "            # normalise while preserving '*' (we don't expect '*' inside tokens)\n",
    "            tok = t.translate(TRANSLATOR).lower()\n",
    "            if tok:\n",
    "                cleaned.append(tok)\n",
    "\n",
    "    words = []\n",
    "    gaps = []\n",
    "    gap = 0\n",
    "\n",
    "    for t in cleaned:\n",
    "        if t == \"*\":\n",
    "            gap += 1\n",
    "        else:\n",
    "            if words:\n",
    "                gaps.append(gap)\n",
    "            words.append(t)\n",
    "            gap = 0\n",
    "\n",
    "    return words, gaps\n",
    "\n",
    "def get_candidate_documents(words: list[str], source_label: str) -> set:\n",
    "    \"\"\"\n",
    "    Candidate docs must contain ALL query words.\n",
    "    We intersect doc-id sets per word, starting from the smallest set.\n",
    "    \"\"\"\n",
    "    doc_sets = []\n",
    "    for w in words:\n",
    "        postings = index.get(w)\n",
    "        if not postings:\n",
    "            return set()\n",
    "        docs = set(postings[source_label].keys())\n",
    "        if not docs:\n",
    "            return set()\n",
    "        doc_sets.append(docs)\n",
    "\n",
    "    doc_sets.sort(key=len)\n",
    "    candidates = doc_sets[0]\n",
    "    for s in doc_sets[1:]:\n",
    "        candidates &= s\n",
    "        if not candidates:\n",
    "            break\n",
    "    return candidates\n",
    "\n",
    "def document_matches_pattern(source_label: str, document_id, words: list[str], gaps: list[int]) -> bool:\n",
    "    \"\"\"\n",
    "    Positional match for patterns like:\n",
    "    word1 * * word2 * word3\n",
    "    where each '*' represents exactly one token.\n",
    "    \"\"\"\n",
    "    # Fetch position lists; if any term missing in this doc, fail fast\n",
    "    position_lists = []\n",
    "    for w in words:\n",
    "        positions = index[w][source_label].get(document_id)\n",
    "        if not positions:\n",
    "            return False\n",
    "        position_lists.append(positions)\n",
    "\n",
    "    # Try each occurrence of the first word as a start\n",
    "    first_positions = position_lists[0]\n",
    "    for start_pos in first_positions:\n",
    "        cur_pos = start_pos\n",
    "        ok = True\n",
    "\n",
    "        for i in range(1, len(position_lists)):\n",
    "            expected = cur_pos + gaps[i - 1] + 1\n",
    "            next_positions = position_lists[i]\n",
    "\n",
    "            # Fast fail if expected is beyond last occurrence\n",
    "            if expected > next_positions[-1]:\n",
    "                ok = False\n",
    "                break\n",
    "\n",
    "            # Binary search would be ideal, but list lengths are usually small.\n",
    "            # Use a pointer scan to find expected.\n",
    "            j = 0\n",
    "            while j < len(next_positions) and next_positions[j] < expected:\n",
    "                j += 1\n",
    "\n",
    "            if j == len(next_positions) or next_positions[j] != expected:\n",
    "                ok = False\n",
    "                break\n",
    "\n",
    "            cur_pos = expected\n",
    "\n",
    "        if ok:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def wildcard_search(query_text: str) -> dict:\n",
    "    words, gaps = parse_query(query_text)\n",
    "    if not words:\n",
    "        return {\"article\": [], \"tweet\": []}\n",
    "\n",
    "    candidate_articles = get_candidate_documents(words, \"article\")\n",
    "    candidate_tweets = get_candidate_documents(words, \"tweet\")\n",
    "\n",
    "    matched_articles = [doc_id for doc_id in candidate_articles\n",
    "                        if document_matches_pattern(\"article\", doc_id, words, gaps)]\n",
    "    matched_tweets = [doc_id for doc_id in candidate_tweets\n",
    "                      if document_matches_pattern(\"tweet\", doc_id, words, gaps)]\n",
    "\n",
    "    matched_articles.sort()\n",
    "    matched_tweets.sort()\n",
    "\n",
    "    return {\"article\": matched_articles, \"tweet\": matched_tweets}\n",
    "\n",
    "def print_search_results(query: str, results: dict, preview_n: int = 10) -> None:\n",
    "    \"\"\"Skimmable output: counts + a small preview.\"\"\"\n",
    "    a = results[\"article\"]\n",
    "    t = results[\"tweet\"]\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Articles matched: {len(a)}\")\n",
    "    if a:\n",
    "        print(\"Article IDs (preview):\", a[:preview_n], \"...\" if len(a) > preview_n else \"\")\n",
    "\n",
    "    print(f\"Tweets matched: {len(t)}\")\n",
    "    if t:\n",
    "        print(\"Tweet IDs (preview):\", t[:preview_n], \"...\" if len(t) > preview_n else \"\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Demo queries (clean output)\n",
    "print_search_results(\"new * uk\", wildcard_search(\"new * uk\"))\n",
    "print_search_results(\"institute * * * university * * university * university\",\n",
    "                     wildcard_search(\"institute * * * university * * university * university\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ef17e-df21-4461-8b6b-6d864eb739c5",
   "metadata": {},
   "source": [
    "## Summary and future directions\n",
    "\n",
    "This notebook demonstrated how large text corpora can be processed efficiently using core Python only, with an emphasis on streaming I/O, careful token normalisation, and appropriate use of built-in data structures.\n",
    "\n",
    "Key themes included:\n",
    "- Streaming-based processing of large, line-delimited JSON datasets to minimise memory usage.\n",
    "- Use of interpretable similarity measures (Jaccard index) and simple pruning strategies to reduce unnecessary computation.\n",
    "- Construction of a shared positional inverted index to support efficient phrase and wildcard-style search across corpora.\n",
    "\n",
    "Possible extensions include:\n",
    "- More robust tokenisation for social media text (e.g. URLs, emojis, hashtags).\n",
    "- Alternative similarity measures such as TF–IDF with cosine similarity.\n",
    "- Persisting the inverted index to disk for larger-than-memory datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8113a5b-9b44-497b-91da-140b0bbd0c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
